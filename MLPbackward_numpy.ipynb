{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T22:47:13.982074Z",
     "start_time": "2018-09-29T22:47:13.672095Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(y_pred, y_true):\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    order = np.argsort(y_pred)\n",
    "    plt.plot(y_true[order], 'o ', label=u'réel')\n",
    "    plt.plot(y_pred[order], 'P ', label=u'prédiction')\n",
    "    plt.legend()\n",
    "    plt.xlabel(u'échantillons')\n",
    "    plt.ylabel('valeur de y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization d'un MLP\n",
    "\n",
    "Dans cet exercice nous allons réaliser l'optimization d'un perceptron à une couche caché. \n",
    "On se servira de pytorch et sa fonctionalité autograd pour apliquer la descente du gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données : Predire la progression du diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On continuera à travailler avec les données sur la progression du diabetes. Le code ci dessus charge les données dans des variables, les normalize et les divide en \"train\" et \"test\", exactement comme dans le notebook antérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "\n",
    "#print(data.DESCR)\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(X[:10,:], columns=data.feature_names)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "y_train, y_test = y_train.reshape([-1,1]), y_test.reshape([-1,1])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP en numpy\n",
    "\n",
    "Ici je reprends le code pour le MLP qui à été fait dans le notebook antérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[0]  # nombre d'échantillons\n",
    "D = X_train.shape[1]  # nombre de features\n",
    "S = y_train.shape[1]  # nombre de sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(D, S, N=10):\n",
    "    # N le nombre de neurones de la couche caché\n",
    "    rng = np.random.RandomState(0)\n",
    "    \n",
    "    # changez les tailles des matrices ici pour leurs valeurs correctes\n",
    "    # tailleW = (0,0)\n",
    "    # tailleb = (0,0)\n",
    "    # tailleO = (0,0)\n",
    "    tailleW = (D,N)\n",
    "    tailleb = (N,1)\n",
    "    tailleO = (N,S)\n",
    "  \n",
    "    W = rng.rand(*tailleW)\n",
    "    b = np.zeros(tailleb)\n",
    "    \n",
    "    O = rng.rand(*tailleO)\n",
    "    \n",
    "    return W, b, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X, W, b, O):\n",
    "    M, D = X.shape  # nombre d'échantillons, nombre de features\n",
    "    N = W.shape[1]  # N le nombre de neurones de la couche caché\n",
    "    \n",
    "    # écrivez le calcul de H\n",
    "    # H = np.zeros(1) # changez ici pour avoir la bonne equation de H\n",
    "    # A = np.tanh(H)  \n",
    "    H = np.dot(X,W) + b.T # changez ici pour avoir la bonne equation de H\n",
    "    A = np.tanh(H)  \n",
    "\n",
    "    # écrivez le calul de Y\n",
    "    # Y = np.zeros(1) # changez ici pour avoir la bonne equation de Y\n",
    "    Y = np.dot(A,O) # changez ici pour avoir la bonne equation de Y\n",
    "    \n",
    "    # Ici quelques verifications sur la taille des matrices pour vous aider\n",
    "    try:\n",
    "        assert(H.shape == (M,N))\n",
    "    except AssertionError:\n",
    "        print(\"Taille de H semble erronée:\",H.shape, \", ça devrait être\", (M,N))    \n",
    "\n",
    "    try:\n",
    "        assert(Y.shape == (M,S))\n",
    "    except AssertionError:\n",
    "        print(\"Taille de Y semble erronée:\",Y.shape, \", ça devrait être\", (M,S))\n",
    "    \n",
    "    return Y, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cout(Y_pred, Y_true):\n",
    "    M = Y_true.shape[0]\n",
    "    # completez le code avec l'expression pour la fonction de cout J\n",
    "    # J = 0 # changez ici pour avoir la bonne equation de J\n",
    "    J = (1/2)*np.mean((Y_pred - Y_true)**2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, O = parameters(D, S, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred, A = MLP(X_train, W, b, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = cout(Y_pred, y_train)\n",
    "J"
   ]
  },
  {
   "source": [
    "## Le gradient de J\n",
    "Comme pour la regression lineaire, il nous faudra calculer les gradients de J envers les poids et biais du reseau. \n",
    "$$grad_O J = A^T (Y-\\hat{Y})$$\n",
    "$$grad_b = \\mathbb{1}_M^T \\left[(Y-\\hat{Y}) O^T \\odot (1-A^2)\\right] $$\n",
    "$$grad_W = X^T  \\left[(Y-\\hat{Y}) O^T \\odot (1-A^2)\\right]  $$\n",
    "$$A^2:=\\{(a_i^k)^2\\}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, Y_pred, Y_true, W, b, O, A):\n",
    "    dH = 1-A**2\n",
    "    terme_YOA = np.matmul(Y_pred-Y_true, O.T)* dH\n",
    "    gradJb = np.mean(terme_YOA, axis=0, keepdims=True).T\n",
    "    assert(b.shape == gradJb.shape)\n",
    "    \n",
    "    gradJW = # completez\n",
    "    assert(W.shape == gradJW.shape)\n",
    "    \n",
    "    gradJO = # completez\n",
    "    assert(O.shape == gradJO.shape)\n",
    "\n",
    "\n",
    "    return gradJW, gradJb, gradJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "Y_pred, A = MLP(X_train, W, b, O)\n",
    "gradients(X_train, Y_pred, y_train, W, b, O, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : pas du gradient\n",
    "\n",
    "Maintenant vous savez comment calculer les gradients et voir leurs valeurs. A vous de remplir la fonction cidessus avec les mises à jour pour la descente du gradient.\n",
    "Vous allez metre à jour W, b et O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(learning_rate, X, Y_pred, Y_true, W, b, O, A):\n",
    "    lr = learning_rate\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    gradJW, gradJb, gradJO = gradients(X, Y_pred, Y_true, W, b, O, A)\n",
    "\n",
    "    # Metez à jour W, b et O \n",
    "    # dans la direction oposé du gradient\n",
    "    # completez\n",
    "    W -= lr * gradJW\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : boucle d'optimization\n",
    "Tout est prêt pour l'entrainement de notre réseau. Il faut maintenant créer la boucle d'optimization qui realise la descente de gradient pour W, b, et O. Suivez les indications et completez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W, b, O = parameters(D, S, N=5) # N est le nombre de neurones de la couche caché\n",
    "max_iterations = 1000\n",
    "learning_rate = 1e-5\n",
    "cost_curve = []\n",
    "for i in range(max_iterations):\n",
    "    #Completez le code ci dessous (numeros 1 a 3)\n",
    "    # 1) calculez les predictions avec le réseau (forward pass)\n",
    "    Y_pred, A =\n",
    "    \n",
    "    # 2) Calculez la fonction de cout\n",
    "    J = \n",
    "    \n",
    "    # Sauvegarder J pour plot\n",
    "    cost_curve.append(J)\n",
    "    \n",
    "    # 3) Calculez les gradients et metez a jour W, b et o avec la fonction gradient_step\n",
    "    gradient_step\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tout se passe bien vous allez voir ci-dessus l'evolution de la valeur de la focntion de coût au long des iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_curve)\n",
    "plt.title(\"courbe d'apprentissage\")\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y(Y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du modèle\n",
    "\n",
    "Finalement, on poura tester la qualité de notre modéle sur notre ensemble de teste ( qui n'a pas été utilisé pour l'apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred, A = MLP(X_test, W, b, O)\n",
    "J = cout(Y_pred,y_test)\n",
    "print(\"cout sur l'ensemble de test\", J)\n",
    "plot_y(Y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : essayez d'augmenter le nombre d'iterations, de neurones ou changer la learning rate pour voir si le modèle peut mieux faire !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrigés\n",
    "### Gradients\n",
    "``` python\n",
    "def gradients(X, Y_pred, Y_true, W, b, O, A):\n",
    "    dH = 1-A**2\n",
    "    terme_YOA = np.matmul(Y_pred-Y_true, O.T)* dH\n",
    "    gradJb = np.mean(terme_YOA, axis=0, keepdims=True).T\n",
    "    assert(b.shape == gradJb.shape)\n",
    "    \n",
    "    # gradJW = # completez\n",
    "    gradJW = np.matmul(X.T, terme_YOA)\n",
    "    assert(W.shape == gradJW.shape)\n",
    "    \n",
    "    # gradJO = # completez\n",
    "    gradJO = np.matmul(A.T, (Y_pred-Y_true))\n",
    "    assert(O.shape == gradJO.shape)\n",
    "\n",
    "    return gradJW, gradJb, gradJO\n",
    "```\n",
    "### Gradient step\n",
    "``` python\n",
    "def gradient_step(learning_rate, W,b,O):\n",
    "    lr = learning_rate\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    gradJW, gradJb, gradJO = gradients(Y_pred, Y_true, W, b, O, A)\n",
    "\n",
    "    # Metez à jour W, b et O \n",
    "    # dans la direction oposé du gradient\n",
    "    # completez\n",
    "    W -= lr * gradJW\n",
    "    b -= lr * gradJb\n",
    "    O -= lr * gradJO\n",
    "\n",
    "```    \n",
    "### Boucle d'entrainement\n",
    "``` python\n",
    "for i in range(max_iterations):\n",
    "    #Completez le code ci dessous (numeros 1 a 3)\n",
    "    # 1) calculez les predictions avec le réseau (forward pass)\n",
    "    Y_pred, A = MLP(W, b, O)\n",
    "    \n",
    "    # 2) Calculez la fonction de cout\n",
    "    J = cout(Y_pred, y_train)\n",
    "    \n",
    "    # Sauvegarder J pour plot\n",
    "    cost_curve.append(J)\n",
    "    \n",
    "    # 3) Calculez les gradients et metez a jour W, b et o avec la fonction gradient_step\n",
    "    gradient_step(learning_rate, Y_pred, Y_true, W, b, O, A)  \n",
    "```     \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "name": "python3810jvsc74a57bd07dba6e18e6acdf8fcfcaa1bbab0f558cda8ecf0f594e6f632ac4fb60f19865cf",
   "display_name": "Python 3.8.10 64-bit ('CoursNNDL': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "theme": "simple",
   "transition": "none"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "327.009px",
    "width": "355.999px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.997px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.085px",
    "left": "1310.04px",
    "right": "20px",
    "top": "161.992px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "7dba6e18e6acdf8fcfcaa1bbab0f558cda8ecf0f594e6f632ac4fb60f19865cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}