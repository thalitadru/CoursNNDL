{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcfd12b-962b-471a-af98-a86770a2c8d6",
   "metadata": {},
   "source": [
    "# Classification d'images de fleurs\n",
    "\n",
    "Sur ce notebook on va parcourir toutes les étapes pour implémenter un réseau convolutif qui fait de la classification d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c2723-18c2-43cf-88eb-f20e3d4b9b2a",
   "metadata": {},
   "source": [
    "## Imports de modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb9388-dc99-4c99-9625-e76fbc92384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "SEED = 123\n",
    "plt.style.available\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "plt.style.use(\"seaborn-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abf86d-6983-4a77-b7ee-c2ac3a843f0f",
   "metadata": {},
   "source": [
    "## Les données\n",
    "Pour entraîner notre modèle, on va avoir besoin d'images de fleurs étiquetés. On s'utilisera de la base d'exemple [tf_images](https://www.tensorflow.org/datasets/catalog/tf_flowers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7afb9-b4c5-48eb-9a16-89e37519fb43",
   "metadata": {},
   "source": [
    "On peut explorer le dataset à l'aide de l'outil suivant:\n",
    "- [Exploration du dataset avec KnowYourData](https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=tf_flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc8c28-4301-4a8c-b411-023b3ca2ad6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Téléchargement des données\n",
    "\n",
    "De toute façon, on devra télécharger les donnés sur notre espace de travail pour dévélopper le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbe157-bf4d-4c46-ab8f-6555b89128d5",
   "metadata": {
    "id": "57CcilYSG0zv"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(\"flower_photos\", origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1def58-a219-4167-bdb3-bda432305edc",
   "metadata": {
    "id": "DPHx8-t-VrVo"
   },
   "source": [
    "Le dataset contient 3670 photos de fleurs, organisees en 5 dossiers:\n",
    "\n",
    "```\n",
    "flower_photo/\n",
    "  daisy/\n",
    "  dandelion/\n",
    "  roses/\n",
    "  sunflowers/\n",
    "  tulips/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d95452-4e07-4bd5-91b1-c4b83f30850a",
   "metadata": {
    "id": "SbtTDYhOHZb6"
   },
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob(\"*/*.jpg\")))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af194c-a998-4bca-bd3b-823dc254bc67",
   "metadata": {},
   "source": [
    "### Génération des datasets train et validation\n",
    "\n",
    "On gardera 20% des données pour la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2c7ac-6c83-45bd-a3c1-1a605a780bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_pixels = 180  # @param {type: \"number\"}\n",
    "batch_size = 32  # @param {type: \"number\"}\n",
    "img_height, img_width = taille_pixels, taille_pixels\n",
    "img_size = (img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f26ab2-759d-4a79-b6fe-624d6536fd8e",
   "metadata": {
    "id": "fIR0kRZiI_AT"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0de9e-c48e-4579-b4e2-9cf6842f8cc8",
   "metadata": {
    "id": "iscU3UoVJBXj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b27281-1efd-455a-8f5f-33169e1a87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff8776-73a3-4bad-805c-5dd3e4a8dee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### L'objet dataset\n",
    "Les objets retournés fonctionnent comme un `iterator` Python. C'est à dire qu'on peut l'utiliser directement dans une boucle for comme ici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86947448-7b6c-4cb4-bd0c-22decb902080",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in train_dataset:\n",
    "    print(\"type:\", type(element), \"taille:\", len(element))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfacb33b-9bd3-4ce1-977a-a0ffa731be30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055eb36-48e1-4c09-ba68-8bb2a0d5dbe5",
   "metadata": {},
   "source": [
    "Les éléments qu'on accède à chaque fois dans la boucle sont des tuples avec deux éléments. Le premier est une image, et le deuxième son étiquette. On peut les séparer dans la boucle ainsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd40ef-4368-4a4a-8f06-911867154959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset:\n",
    "    print(image.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bbed1-b227-4d42-b255-4544e098669c",
   "metadata": {},
   "source": [
    "### Exploration rapide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b35916-9984-4cb4-912c-2b4d1725dd70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regardons quelques images\n",
    "\n",
    "Ici un bout de code pour vous afficher quelques images de la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bf417-5cdb-469b-817f-29e2e1ee82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Quelques images du train set { display-mode: \"form\" }\n",
    "max_images = \"9\"  # @param [\"4\", \"9\", \"16\"]\n",
    "max_images = int(max_images)\n",
    "count = 0\n",
    "gs = grid_size = int(np.sqrt(max_images))\n",
    "fig, axs = plt.subplots(gs, gs, sharex=True, sharey=True, figsize=(gs * 3, gs * 3))\n",
    "axs = axs.flatten()\n",
    "for image, label in train_dataset:\n",
    "    ax = axs[count]\n",
    "    ax.imshow(image[0].numpy().astype(np.uint8))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(str(label[0].numpy()))\n",
    "    count += 1\n",
    "    if count >= max_images:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bf514-6209-4df9-a373-997bc0e28d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Quelques images de valdation { display-mode: \"form\" }\n",
    "max_images = \"4\"  # @param [\"4\", \"9\", \"16\"]\n",
    "max_images = int(max_images)\n",
    "count = 0\n",
    "gs = grid_size = int(np.sqrt(max_images))\n",
    "fig, axs = plt.subplots(gs, gs, sharex=True, sharey=True, figsize=(gs * 3, gs * 3))\n",
    "axs = axs.flatten()\n",
    "for image, label in val_dataset:\n",
    "    ax = axs[count]\n",
    "    ax.imshow(image[0].numpy().astype(np.uint8))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(str(label[0].numpy()))\n",
    "    count += 1\n",
    "    if count >= max_images:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97ee24-b01d-42ac-9714-c9949f3a4e75",
   "metadata": {},
   "source": [
    "#### Histogramme des étiquettes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baeec61-32b2-438b-929c-d5cc40e00592",
   "metadata": {},
   "source": [
    "On accumule les étiquettes dans une liste et puis on la transforme en array numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686f699-62ed-4064-93f1-de65038c044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = np.arange(num_classes, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e5bbe-3e91-432e-b609-9ff8c5406b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for image, label in train_dataset:\n",
    "    labels.append(label.numpy())\n",
    "\n",
    "train_labels = np.concatenate(labels)\n",
    "print(\"Shape de l'array labels: \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a76bac-b47b-4716-bfc2-cba21d24bf20",
   "metadata": {},
   "source": [
    "On fait le même pour l'ensemble de validation pour vérifier si la distribution est semblable à celle de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e3bc7-6481-4778-bee6-91e1ad8fec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for image, label in val_dataset:\n",
    "    labels.append(label.numpy())\n",
    "val_labels = np.concatenate(labels)\n",
    "print(\"Shape de l'array labels: \", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfadf0-6703-4de0-b916-6f0218955b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Distribution des classes { display-mode: \"form\" }\n",
    "plt.hist(\n",
    "    train_labels,\n",
    "    bins=num_classes,\n",
    "    rwidth=0.8,\n",
    "    align=\"left\",\n",
    "    label=\"train\",\n",
    "    density=True,\n",
    ")\n",
    "plt.hist(\n",
    "    val_labels,\n",
    "    bins=num_classes,\n",
    "    rwidth=0.8,\n",
    "    align=\"mid\",\n",
    "    label=\"valid\",\n",
    "    density=True,\n",
    ")\n",
    "plt.xticks(label_values, label_values)\n",
    "plt.legend()\n",
    "plt.title(\"distribution des categories sur les partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523df2b-d4d8-40f4-b82c-3254b6addbf6",
   "metadata": {},
   "source": [
    "#### Les valeurs des pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d87e4f-f3a6-4c8c-b0d4-3bc970d375c5",
   "metadata": {},
   "source": [
    "Remarquez que les valeurs des pixels RGB varient entre 0 et 255. \n",
    "Pour les réseaux de neuronnes, il vaut mieux garder les valeurs d'entrée petites, come par exemple entre [0,1]. Il nous faudra donc diviser la valeur des pixels par 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df7b44-a522-40a6-bddc-9b74fd0fe7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Affichage des 3 canaux individuellement { display-mode: \"form\" }\n",
    "for image, label in train_dataset:\n",
    "    img = image[0].numpy()\n",
    "    cmaps = [\"Reds\", \"Greens\", \"Blues\"]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 3))\n",
    "    axs[3].imshow(img.astype(\"uint8\"))\n",
    "    ax.set_title(\"Image RBG\")\n",
    "    for ch in range(3):\n",
    "        ax = axs[ch]\n",
    "        imgplt = ax.imshow(img[..., ch], cmap=cmaps[ch])\n",
    "        ax.set_title(\"Canal \" + cmaps[ch][0])\n",
    "        fig.colorbar(imgplt, ax=ax)\n",
    "    break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3c10f-5a04-488b-bc74-a49254c37c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title def de la fonction image_hist { display-mode: \"form\" }\n",
    "def image_hist(img):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "    ch_name = [\"rouge\", \"vert\", \"bleu\"]\n",
    "    color = \"rgb\"\n",
    "    for i in range(3):\n",
    "        channel = img[..., i]\n",
    "        ax = axs[i]\n",
    "        ax.hist(\n",
    "            channel.flatten(),\n",
    "            orientation=\"horizontal\",\n",
    "            color=color[i],\n",
    "            range=(0, 256),\n",
    "        )\n",
    "        ax.set_title(ch_name[i])\n",
    "        ax.xaxis.set_visible(False)\n",
    "    fig.suptitle(\"histogramme des pixels d'une image\")\n",
    "    ax = axs[-1]\n",
    "    ax.imshow(img.astype(\"uint8\"))\n",
    "    ax.axis(\"off\")\n",
    "    print(\"pixels: max=\", img.max(), \"min=\", img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f66b08-5295-4a49-9264-d90f9ee14800",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset:\n",
    "    img = image[0].numpy()\n",
    "    image_hist(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6f126-442d-4336-864c-e14c6656568d",
   "metadata": {},
   "source": [
    "## Le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46f9fd-6536-4b26-9f3f-4237fbcaf760",
   "metadata": {},
   "source": [
    "### Création du modèle `Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617665a1-f2e7-4f46-a948-6dd649ed6280",
   "metadata": {
    "id": "QR6argA1K074"
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.class_names)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=(img_height, img_width, 3), name=\"input\"),\n",
    "        layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\", name=\"conv1\"),\n",
    "        layers.MaxPooling2D(name=\"pool1\"),\n",
    "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", name=\"conv2\"),\n",
    "        layers.MaxPooling2D(name=\"pool2\"),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv3\"),\n",
    "        layers.MaxPooling2D(name=\"pool3\"),\n",
    "        layers.Flatten(name=\"pool3_flat\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"dense4\"),\n",
    "        layers.Dense(num_classes, name=\"output\"),\n",
    "    ],\n",
    "    name=\"cnn_flowers\",\n",
    ")\n",
    "model.save_weights(\"init.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9f2c9-9cc6-4b23-8dbb-4fbc5e179f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197d979-e3f4-48d8-9f9f-7147a2c4f6a1",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb6b7b-2f1c-4469-b167-452aab3095d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_layer_names=False, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bd65a-0636-46cf-ab2b-d761c189522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_layer_names=False, show_shapes=True, rankdir=\"TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afb4fd-6249-4dcd-90d7-df9c46019f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import visualkeras as vk\n",
    "except ModuleNotFoundError:\n",
    "    !pip install visualkeras;\n",
    "    import visualkeras as vk\n",
    "\n",
    "vk.layered_view(model, legend=True, scale_xy=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ab391-1645-479e-9289-b08812bd55a0",
   "metadata": {},
   "source": [
    "## L'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b178043-0dfa-4d03-a76c-17cdab20453c",
   "metadata": {},
   "source": [
    "### Lecture rapide des batchs\n",
    "Pour que la lecture des `batch`s de données soit plus efficace, `tf.Dataset` nous fournit quelques méthodes adaptés. Elles nous transforment l'objet originel dans un autre avec une fonctionnalité en plus.\n",
    "\n",
    "Ici on utilisera:\n",
    "- cache: garder en mémoire les images une fois chargées\n",
    "- prefetch: pre-charger un certain nombre de batches en mémoire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a8f6d-8702-46d7-9428-32ed0b62aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8636eb-d8c9-4cf4-8ee2-e35803b2c240",
   "metadata": {},
   "source": [
    "### `compile` et `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5516f9f-18fc-449b-a7f5-f4b6f0a1adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cf689-9712-49e7-b4b7-366c9b757af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training loop\n",
    "reinit = True  # @param {type: \"boolean\"}\n",
    "epochs = 10  # @param {type: \"number\"}\n",
    "learning_rate = 0.001  # @param {type: \"number\"}\n",
    "\n",
    "model.optimizer.learning_rate = learning_rate\n",
    "\n",
    "if reinit:\n",
    "    model.load_weights(\"init.h5\")\n",
    "\n",
    "out = model.fit(train, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607c8d4-c9db-4ccf-bbe7-9214753faa07",
   "metadata": {},
   "source": [
    "### Visualiser les courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe433636-d52b-4d2f-ba60-5ba5c4bfef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Def de la fonction learning_curves { display-mode: \"form\" }\n",
    "def learning_curves(out, **kwargs):\n",
    "    metrics = out.history.keys()\n",
    "    # collecter les noms des metriques enregistrés sur history\n",
    "    metrics = list(filter(lambda m: not m.startswith(\"val\"), metrics))\n",
    "    num_metrics = len(metrics)\n",
    "    # creer un gid de subplots approprié\n",
    "    fig, axs = plt.subplots(\n",
    "        num_metrics,\n",
    "        1,\n",
    "        figsize=(9, 4 * num_metrics),\n",
    "    )\n",
    "    # afficher chaque type de métrique dans son plot\n",
    "    for i, metric in enumerate(metrics):\n",
    "        loss = out.history[metric]\n",
    "        val_loss = out.history[\"val_\" + metric]\n",
    "        epochs_range = range(1, len(loss) + 1)\n",
    "        ax = axs[i]\n",
    "        ax.plot(\n",
    "            epochs_range,\n",
    "            loss,\n",
    "            marker=\".\",\n",
    "            linestyle=\"dashed\",\n",
    "            label=\"Train \" + metric,\n",
    "            **kwargs\n",
    "        )\n",
    "        ax.plot(\n",
    "            epochs_range,\n",
    "            val_loss,\n",
    "            marker=\".\",\n",
    "            linestyle=\"dashed\",\n",
    "            label=\"Valid \" + metric,\n",
    "            **kwargs\n",
    "        )\n",
    "        ax.legend()\n",
    "        ax.set_title(metric)\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    fig.suptitle(\"courbes d'apprentisage x époques\", fontsize=\"x-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c86f0f-f515-4566-8957-3711db57235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ebaac-585d-441a-b650-60035a433f72",
   "metadata": {},
   "source": [
    "## Diagnostique : sur-apprentissage\n",
    "\n",
    "On peut remarquer sur les courbes d'apprentissage:\n",
    "- un erreur d'entraînement quand même très bas;\n",
    "- un grand écart entre les métriques `train` et `val`, ce qui indique un grand erreur de généralisation;\n",
    "\n",
    "Ses symptomes indiquent que le modèle souffre de sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0382ee-b0a5-4206-838c-bbf5eddd931e",
   "metadata": {},
   "source": [
    "## Solutions pour un modèle 2.0\n",
    "Pour l'ameliorer, on peut prendre certaines mesures comme:\n",
    "\n",
    "- augmenter la quantité de données\n",
    "- réduire la complexité du modèle\n",
    "- introduire de la régularisation\n",
    "\n",
    "On va ici utiliser deux strategies:\n",
    "- le _data augmentation_: une augmentation artificielle des données\n",
    "- le _dropout_: une technique de régularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9929bf-8c7d-4317-9c6e-e4da5b3fd72d",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f505625-0b04-4571-a8fe-6be12bc6e2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=(img_height, img_width, 3)),\n",
    "        layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.1),\n",
    "        layers.experimental.preprocessing.RandomZoom(height_factor=0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50441655-0ac9-42d1-a6fb-14efd6491350",
   "metadata": {
    "id": "7Z90k539S838"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b85bc1-aa84-46aa-a211-7e0ae5f61b57",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a42d0e-d8e3-49f7-9e5c-b02955d9321d",
   "metadata": {
    "id": "2Zeg8zsqXCsm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = Sequential(\n",
    "    [\n",
    "        data_augmentation,\n",
    "        layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\", name=\"conv1\"),\n",
    "        layers.MaxPooling2D(name=\"pool1\"),\n",
    "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", name=\"conv2\"),\n",
    "        layers.MaxPooling2D(name=\"pool2\"),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv3\"),\n",
    "        layers.MaxPooling2D(name=\"pool3\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(name=\"pool3_flat\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"dense4\"),\n",
    "        layers.Dense(num_classes, name=\"output\"),\n",
    "    ],\n",
    "    name=\"cnn_flowers2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb95d9e-ba17-410b-8253-fa419e2ff799",
   "metadata": {
    "id": "2Zeg8zsqXCsm",
    "tags": []
   },
   "source": [
    "Suite à l'inclusion des couches d'augmentation, on a des changements dans le backend qui font que l'on doive faire un appel a la méthode `build` ou appeler le réseau sur un batch d'échantillons pour faire initialiser ses poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece9118-d5e2-486b-a7cb-2a09ed333854",
   "metadata": {
    "id": "2Zeg8zsqXCsm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.build(input_shape=(batch_size, img_height, img_width, 3))\n",
    "# ou \n",
    "# model2(rng.rand(batch_size, img_height, img_width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841031e6-d190-4f41-b031-032b285310c9",
   "metadata": {},
   "source": [
    "Maintenant on peut les sauvegarder sous un nouveau nom à fin de ne pas écraser celui du précédent réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064d3be-ad5b-4d97-848a-010fca1eedc3",
   "metadata": {
    "id": "2Zeg8zsqXCsm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.save_weights(\"init2.0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95eb859-a936-4fbe-a8b6-6299db4f19b8",
   "metadata": {},
   "source": [
    "## Entraînement 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a2d9e-5bde-4fa1-add9-5855de7dc4f8",
   "metadata": {},
   "source": [
    "### `compile` et `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fff18-2e10-4997-be36-e4303dec4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373357d2-334a-4f4d-8555-c95ac6ca0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training loop\n",
    "reinit = True  # @param {type: \"boolean\"}\n",
    "epochs = 15  # @param {type: \"number\"}\n",
    "learning_rate = 0.001  # @param {type: \"number\"}\n",
    "\n",
    "model2.optimizer.learning_rate = learning_rate\n",
    "\n",
    "if reinit:\n",
    "    model2.load_weights(\"init2.0.h5\")\n",
    "\n",
    "out2 = model2.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20208b1-f297-4073-af0a-729837534a5f",
   "metadata": {},
   "source": [
    "### Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d181dbb-a212-4457-9ccc-d4ec1b956285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_curves(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e9b6-e6f1-4c6d-872f-61a9e46aac18",
   "metadata": {},
   "source": [
    "**Optionnel**: Exécutez le code suivant si vous voulez sauvegarder votre `model2` dans un fichier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7e08f-5306-45ac-892f-745cbe971c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.save('model2.0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa11fc5-915c-453d-995b-5fc48e7a345f",
   "metadata": {},
   "source": [
    "## Test final\n",
    "On essaiera de faire des prédictions sur d'autres images de fleurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cb42d-88f1-445a-9d08-d8b13a1cd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\",\n",
    "    \"https://d2j6dbq0eux0bg.cloudfront.net/images/9350281/982837658.jpg\",\n",
    "    \"https://www.roses-andre-eve.com/2811-large_default/aspirin-rose-taniripsa-.jpg\",\n",
    "    \"https://cdn11.bigcommerce.com/s-f74ff/images/stencil/1280x1280/products/9531/29981/wetland-plants-dandelion-plant-dandelion__86787.1600975395.jpg?c=2\",\n",
    "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzXukWgU6XFSKQWMLkE-ZVnuTCbQESSWMYvQ&usqp=CAU\",\n",
    "    \"https://www.rhs.org.uk/getmedia/207b5a2a-2332-4863-8990-070175545f92/Tulip-Fusilier-C-Dorling-Kindersley.jpg?width=940&height=627&ext=.jpg\",\n",
    "    \"https://ak.picdn.net/shutterstock/videos/3242563/thumb/1.jpg\",\n",
    "    \"https://www.josephotos.com/wp-content/uploads/2014/03/DSC_7255_daisies.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/3b/Wooden_Shoe_Tulip_Farm_2011_-_Oregon_%285649088598%29.jpg\",\n",
    "]\n",
    "names = [\n",
    "    \"Sunflower\",\n",
    "    \"Rainbow-rose\",\n",
    "    \"Rose-claire\",\n",
    "    \"Dandelion\",\n",
    "    \"Close-Dandelion\",\n",
    "    \"Tulipes-ouvertes\",\n",
    "    \"daisy-field\",\n",
    "    \"daisy-top\",\n",
    "    \"tulip-field\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0178ca5-e000-4f3c-b2a6-8813601f7285",
   "metadata": {},
   "source": [
    "**Optionnel**: Exécutez le code suivant si vous avez sauvegardé un modèle et que vous voulez le charger pour l'utiliser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73c88b-252b-4332-afa2-73444e2b9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = keras.models.load_model('model2.0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250060f-3ef4-4451-aaa9-09f0d619fcce",
   "metadata": {},
   "source": [
    "Ici le code télécharge les images choisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cd33e-3a9c-4f7c-b430-2f5f556eeb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "for url, name in zip(urls, names):\n",
    "    img_path = tf.keras.utils.get_file(name, origin=url)\n",
    "    img_paths.append(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb6b65-1411-46f0-aef8-1f4088d1a142",
   "metadata": {},
   "source": [
    "Ensuite on va remettre ces images sous un format que le réseau va reconnaître (en ce qui concerne sa taille en pixels et sa représentation en forme d'array 3D/4D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69791030-5f1e-4079-a973-519e99e5d7ac",
   "metadata": {
    "id": "dC40sRITBSsQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name, img_path in zip(names, img_paths):\n",
    "    print(\"\\n{}\".format(name))\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        img_path, target_size=(img_height, img_width)\n",
    "    )\n",
    "    plt.style.use(\"default\")\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis(\"off\")\n",
    "    i += 1\n",
    "\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"Image de la classe {} avec  {:.2f}\\% de confiance.\".format(\n",
    "            class_names[np.argmax(score)], 100 * np.max(score)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CoursNNDL",
   "language": "python",
   "name": "coursnndl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
