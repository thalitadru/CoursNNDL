{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tous les notebooks sont accesibles aussi [ici](https://github.com/thalitadru/CoursNNDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(name):\n",
    "    if name == 'train':\n",
    "        y_ = y_train.reshape(-1)\n",
    "        X_ = X_train\n",
    "    elif name== 'test':\n",
    "        y_ = y_test.reshape(-1)\n",
    "        X_ =X_test\n",
    "    else:\n",
    "        print(\"Il faut taper train ou test\")\n",
    "        return \n",
    "    y_pred_ = y_pred(X_).reshape(-1)\n",
    "    order = np.argsort(y_pred_)\n",
    "    plt.plot(y_[order], 'o ', label=u'réel ')\n",
    "    plt.plot(y_pred_[order], 'P ', label=u'prédiction ')\n",
    "    plt.legend()\n",
    "    plt.xlabel(u'échantillons')\n",
    "    plt.ylabel('valeur de y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratique 1 Descente du gradient avec NumPy\n",
    "Ici nous allons implementer la descente du gradient pour un modèle de regression lineaire. On travailera en laguage python, avec des biliothèques qui permenttent de faire du calcul scientifique. Ici on commencera avec numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bases: vecteurs et matrices sur NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy va nous permettre de créer des matrices et vecterus, et de faire des operations mathématiques assez facilement. On peut créer des tableaux de la taille qu'on veut. Un vecteur d'une seulle dimension se fait ainsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce vecteur n'est ni ligne ni colonne, car il a une seule dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut creer un vrai vecteur-ligne de la façon suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et un vecteur colonne de façon similaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une façon plus courte de l'écrire est de l'écrire en ligne et ensuite demander son transposé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la même fonction on peut creer une matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut sauvegarder les matrices et vecteurs dans une variable, pour les utiliser plus tard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "v = np.array([[2,2,2,2]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérifier la taille avec `shape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut verifier la taille d'un vecteur ou matrice en regardant leur variable `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela nous dit que M a 4 lignes et 3 colonnes. L'ordre est toujours ligne puis colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela nous dit que v a 4 lignes et une colonne (donc un vecteur colone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opérations le long d'une ligne ou colonne\n",
    "\n",
    "On peut réaliser des opérations qui combinent les élements des lignes ou des colonnes d'une matrice: la somme `sum`, la moyenne `mean`, la variance `std`, le `max` ou le `min`.\n",
    "\n",
    "Prennons par exemple une matrice de 1s de taille 3x5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones((3,5))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut sommer au long de chaque colonne, on peut faire le suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(A, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut le faire au long de chaque ligne, on utilise `axis=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(A, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même se fait pour les autres opperations. Par example la moyenne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produit de matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi faire un produit de matrices ou de vecteurs. Par exemple, je veux calculer le produit de $v^T M$, je peux le faire avec la fonction `np.dot`. Cela doit nous retourner un vecteur-ligne de taille 1x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v.T, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi faire $M^T v$. Cela doit nous retourner un vecteur-colonne de taille 3x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(M.T, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication élement à élement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre opération utile est la multiplication élément à élement. Pour l'éxemplifier, on va créer d'abord une matrice pleine de 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones((3,3)) #on met ici la taille de la matrice que l'on souhaite, ici 3x3, representé (3,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disons que l'on veuille multiplier la premiere ligne par 1, la deuxiéme par 2, et ainsi de suite. On peut creer un vecteur avec ces coeficients de la façon suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[1,2,3]]).T\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser la mèthode `np.arange` que nous fournit un vecteur avec une suite de valeurs demandée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,3+1)  # le vecteur sera rempli de chiffres de start à stop-1, pour cela il faut dire de 1 a 3+1\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre ici on reçoit un vector de dimension 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le metre en mode ligne ou colone il faut adapter sa taille (`shape`). Dans sa grande dimension, on met un -1, qui dit a numpy de reprendre la dimension di vector, pour la dimension petite, on met un 1. Comme ça on peut creer un vector ligne ou colonne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((1,-1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((-1,1))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on peut réaliser la multiplication de la façon suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que chaque ligne a été multiplié par un des termes du vector. Ça arrive parce que x est en forme colonne. Pour le faire pour chaque ligne on peut transposer x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opérations avec scalaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opérations avec des scalaires sont faites directement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres opérations élement à élement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand on fait des operations entre des matrices ou vecteurs de même taile, cela se fait élement à element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ces informations, on peut déjà partir à implementer la régression lineaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données\n",
    "\n",
    "On va travailler ici avec un jeu de données synthetique simples. Le code ci dessus vous prepare ce jeux de données. Il contient plus de détails que ce qu'on aura le temps de discuter, vous n'avez pas besoin de le comprendre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "rng=np.random.RandomState(0)\n",
    "m = 800\n",
    "std=1\n",
    "X1 = rng.rand(m) \n",
    "X2 = rng.rand(m) \n",
    "#X1, X2 = np.meshgrid(X1, X2)\n",
    "Y = 3*X1 - 5*X2 + 8 \n",
    "Y += std*rng.rand(*Y.shape)\n",
    "\n",
    "X, y = np.stack([X1.reshape(-1), X2.reshape(-1)],axis=1), Y.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier que notre modèle generalise bien pour des cas inconnus, il est important de separer une portion des données qui ne sera pas utilisé dans l'entraînement, qu'on appelle ensemble de test ou validation.  Ceci est fait ci-desous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "y_train, y_test = y_train.reshape([-1,1]), y_test.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec les données generés, je vous afficherais ici le jeu de données qu'on utilisera pour l'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.title(\"x2 contre x1 - y en couleurs\")\n",
    "plt.scatter(*X_train.T, c=y_train.squeeze(), cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"y contre x1\")\n",
    "plt.scatter(X_train[:,0], y_train.squeeze(), c=y_train.squeeze(), cmap=cm.coolwarm)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"y contre x2\")\n",
    "plt.scatter(X_train[:,1], y_train.squeeze(), c=y_train.squeeze(), cmap=cm.coolwarm)\n",
    "plt.xlabel('x2')\n",
    "plt.ylabel('y')\n",
    "plt.gcf().set_size_inches(16,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif ici est de predire la valeur y (couleur) a partir des deux coordonées x1 et x2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice \n",
    "Regardez la dimension de X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc `X_train` et `y_train` pour apprendre notre regression lineaire. Passons à l'écriture du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle\n",
    "\n",
    "Comme discuté, une regression linéaire a la forme suivante\n",
    "$$ y= Xw + b$$\n",
    "\n",
    "On peut l'écrire de la façon suivante. D'abord, selon la taille des données, on crée le vecteur w et le biais b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[0]  # nombre d'échantillons\n",
    "D = X_train.shape[1]  # nombre de features\n",
    "\n",
    "rng=np.random.RandomState(0)\n",
    "w = rng.rand(D,1)\n",
    "b = np.array(0.0)\n",
    "\n",
    "assert w.shape == (2,1)\n",
    "assert b.shape == () \n",
    "\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on peut écrire la formule de prediction. Ici on va utiliser une fonction courte qui s'écrit dans une seule ligne (`lambda` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lambda x: (np.dot(x,w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'utiliser il faut l'appeler avec les données `x` en argument. On utilisera les données d'entrainement `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction de coût\n",
    "\n",
    "On doit aussi évaluer les predictions faites. On doit donc écrire la fonction de coût:\n",
    "$$ J(w_j) = \\frac{1}{2M}\\sum_i (y_I -\\hat{y}_i)^2$$\n",
    "\n",
    "On peut écrire cete fonction de la façon suivante (en profitant des opérations faites élément par élement pour calculer $y_i - \\hat{y}$ por tout $i$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = lambda x, y : (1/2)*np.mean((y_pred(x)-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour `y_pred`, on doit passer les arguments de la fonction quand on l'appele. Ici, `X_train` et `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : Le gradient de J\n",
    "\n",
    "Pour le gradient de J ça sera três similaire. Pour rappel:\n",
    " \n",
    "$$\\nabla_w J = \\frac{1}{M} (X^T(y -\\hat{y}))$$\n",
    "et\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{M}\\sum_i  (y_i -\\hat{y}_i)$$\n",
    "\n",
    "à vous de l'écrire ci desous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradJW = lambda x, y: 0 # écrivez l'expression en termes de x et y\n",
    "# gradJb = lambda x, y: 0 # écrivez l'expression en termes de x et y\n",
    "gradJW = lambda x, y : (1/M) * np.dot(x.T,(y_pred(x)-y))\n",
    "gradJb = lambda x, y : np.mean((y_pred(x)-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = gradJW(X_train, y_train)\n",
    "db = gradJb(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques testes pour verifier le bon format de vos gradients\n",
    "assert dw.shape == w.shape\n",
    "assert db.shape == b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La boucle de descente du gradient\n",
    "\n",
    "Pour ajuster $w$ et $b$ via méthode du gradient, il nous faut itérer en boucle sur le calcul du gradient et l'ajustement de w. Pour cela on s'utilisera de l'instruction `for i in range(max_iterations)` que répet les instructions qui sont dans son intérieur `max_iterations` fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que les predictions ne sont pas tres bones pour l'instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "Completez le code avec les pas de descente du gradient:\n",
    "\n",
    "$$w_j = w_j - \\lambda \\frac{\\partial J}{\\partial w_j}$$\n",
    "$$b = b - \\lambda \\frac{\\partial J}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# lancez ce code si vous avez besoin de reinitializer w et b\n",
    "rng=np.random.RandomState(0)\n",
    "w = rng.rand(D,1)\n",
    "b = np.array(0.0)\n",
    "\n",
    "assert w.shape == (2,1)\n",
    "assert b.shape == () \n",
    "\n",
    "w, w.shape, b, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iterations = 100000 \n",
    "\n",
    "l = 5e-5  # lambda\n",
    "\n",
    "tol = 1e-7\n",
    "\n",
    "J_now = J(X_train, y_train)\n",
    "\n",
    "cost_curve = [J_now]\n",
    "\n",
    "for i in range(max_iterations):\n",
    "\n",
    "    # apliquer le pas du gradient a w et b\n",
    "    # VOTRE CODE ICI\n",
    "    # w = \n",
    "    # b = \n",
    "    w = w - l * gradJW(X_train, y_train)\n",
    "    b = b - l *gradJb(X_train,y_train)\n",
    "    \n",
    "\n",
    "    # verifier la valeur de la fonction de cout J\n",
    "    J_old = J_now\n",
    "\n",
    "    J_now = J(X_train, y_train)\n",
    "    \n",
    "    cost_curve += [J_now]\n",
    "    # verifier conditions d'arrêt selon la valeur de J\n",
    "    if np.isnan(J_now):\n",
    "        break\n",
    "    if i > 3:\n",
    "        if np.abs(J_now)< tol and np.abs(J_now-J_old) < tol and np.abs(J_old-cost_curve[-3]) < tol:\n",
    "            print(\"arret par convergence @ tol=\", tol)\n",
    "            break\n",
    "    \n",
    "\n",
    "    if not i % (max_iterations//10): \n",
    "        print(\"i\", i,\" cout\", J_now)\n",
    "if i >= max_iterations: print(\"arret par max_iterations \", max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici vous pouvez regarder la valeur de la fonction de cout au long des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_curve)\n",
    "plt.ylabel(\"cost\")\n",
    "plt.xlabel(\"iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer le modèle\n",
    "Regardons maintenant la capacité de notre modèle a faire des prédictions sur des données pas vues pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valeur de la fonction de cout sur l'ensemble de test:\", J(X_test, y_test))\n",
    "plot_y('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice : essayez de changer le nombre d'iterations ou changer lambda pour voir si on arrive a un résultat different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparer avec la solution exacte\n",
    "La solution optimale à une régression lineaire peut être calculé de manière analytique comme fait ci dessous:\n",
    "$$X_1 = (X,\\textbf{1})$$\n",
    "$$ (w|b) = (X_1^T X_1)^{-1}X_1^T\\hat{y}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xones = np.concatenate([X_train, np.ones([M,1])], axis=1)\n",
    "wb = np.dot(np.dot(np.linalg.pinv(np.dot(Xones.T,Xones)), Xones.T), (y_train))\n",
    "w = wb[:-1]\n",
    "b = wb[-1]\n",
    "w,w.shape, b, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la descente de gradient nous rapproche de la solution, mais ne trouve pas forcement l'optimum dans un nombre d'iterations limitées.\n",
    "\n",
    "Regardez ici la valeur de la fonction de cout avec cette solution optimale et aussi comment les valeurs predites sont plus proches des vraies valeures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "J(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrigés\n",
    "Voici les bons morceaux de code à remplir pour que la manip fonctionne.\n",
    "\n",
    "### Gradients de J\n",
    "``` python\n",
    "gradJW = lambda x, y : (1/M) * np.dot(x.T,(y_pred(x)-y))\n",
    "gradJb = lambda x, y : np.mean((y_pred(x)-y))\n",
    "```\n",
    "### Pas du gradient\n",
    "``` python\n",
    "for i in range(max_iterations):\n",
    "    \n",
    "    # apliquer le pas du gradient a w et b\n",
    "    # VOTRE CODE ICI\n",
    "    w = w - l * gradJw(X_train, y_train)\n",
    "    b = b - l *gradJb(X_train,y_train)\n",
    "    \n",
    "    ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "kernelspec": {
   "name": "python3810jvsc74a57bd07dba6e18e6acdf8fcfcaa1bbab0f558cda8ecf0f594e6f632ac4fb60f19865cf",
   "display_name": "Python 3.8.10 64-bit ('CoursNNDL': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7dba6e18e6acdf8fcfcaa1bbab0f558cda8ecf0f594e6f632ac4fb60f19865cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}